{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('keys','r') as key_file:\n",
    "    key = key_file.readline().strip()\n",
    "    secret = key_file.readline().strip()\n",
    "    bearer = key_file.readline().strip()\n",
    "\n",
    "client = tweepy.Client(bearer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nike', 'McDonalds']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this a data source, given by any entity that uses the service\n",
    "subjects = []\n",
    "with open('subjects.txt','r') as subjects_file:\n",
    "    subjects = [x.strip() for x in subjects_file.readlines()]\n",
    "\n",
    "subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exemplify\n",
    "\n",
    "query = f'@{subjects[0]}'\n",
    "\n",
    "count = 10\n",
    "\n",
    "search_result = client.search_recent_tweets(query=query,max_results=count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(search_result.data,columns=['id','text'])\n",
    "df['text'] = df['text'].astype(str)\n",
    "df['subject']=\"nike\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "from scipy.special import softmax\n",
    "import csv\n",
    "import urllib.request\n",
    "\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    " \n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "task='sentiment'\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\" # load once\n",
    "\n",
    "tokenizer_sentiment = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "# download label mapping\n",
    "labels=[]\n",
    "mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n",
    "with urllib.request.urlopen(mapping_link) as f:\n",
    "    html = f.read().decode('utf-8').split(\"\\n\")\n",
    "    csvreader = csv.reader(html, delimiter='\\t')\n",
    "labels = [row[1] for row in csvreader if len(row) > 1]\n",
    "\n",
    "# pre trained model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "model.save_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "text = [preprocess(t) for t in list(df.text.values)]\n",
    "encoded_input = tokenizer_sentiment(text, return_tensors='pt', truncation=True, padding=True)\n",
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = output[0].detach().numpy()\n",
    "scores = softmax(scores,axis=1)\n",
    "ranking = np.argsort(scores)\n",
    "ranking = ranking[:][::-1]\n",
    "\n",
    "df['sentiment']=[labels[x] for x in np.argmax(scores,axis=1)]\n",
    "df['negative']=scores[:,0]\n",
    "df['neutral']=scores[:,1]\n",
    "df['positive']=scores[:,2]\n",
    "\n",
    "\n",
    "# for printing\n",
    "# for t in range(scores.shape[0]):\n",
    "#     print(f'text: {df.text.iloc[t]}')\n",
    "#     for i in range(scores[0].shape[0]):\n",
    "#         l = labels[ranking[t][i]]\n",
    "#         s = scores[t][ranking[t][i]]\n",
    "#         print(f\"{i+1}) {l} {np.round(float(s), 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emotion Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "task='emotion'\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\" # load once\n",
    "\n",
    "tokenizer_emotion = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "# download label mapping\n",
    "labels=[]\n",
    "mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n",
    "with urllib.request.urlopen(mapping_link) as f:\n",
    "    html = f.read().decode('utf-8').split(\"\\n\")\n",
    "    csvreader = csv.reader(html, delimiter='\\t')\n",
    "labels = [row[1] for row in csvreader if len(row) > 1]\n",
    "\n",
    "# pre trained model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "model.save_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [preprocess(t) for t in list(df.text.values)]\n",
    "encoded_input = tokenizer_emotion(text, return_tensors='pt', padding=True)\n",
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: Sponsor me @Crocs @Nike https://t.co/SQpbd2x2MC\n",
      "1) anger 0.0923\n",
      "2) sadness 0.0504\n",
      "3) optimism 0.3455\n",
      "4) joy 0.5118\n",
      "text: @PadrinoProxies @wytmerj @hoopszilla_ @ye4us @Nike @adidas https://t.co/LWZQlVZvVa\n",
      "1) optimism 0.0578\n",
      "2) sadness 0.0423\n",
      "3) anger 0.0461\n",
      "4) joy 0.8538\n",
      "text: Bring this back for a LeBron commercial @Nike @KingJames https://t.co/QD8bCFpa2Z\n",
      "1) sadness 0.0196\n",
      "2) anger 0.0265\n",
      "3) optimism 0.0577\n",
      "4) joy 0.8962\n"
     ]
    }
   ],
   "source": [
    "scores = output[0].detach().numpy()\n",
    "scores = softmax(scores,axis=1)\n",
    "ranking = np.argsort(scores)\n",
    "ranking = ranking[:][::-1]\n",
    "\n",
    "df['emotion']=[labels[x] for x in np.argmax(scores,axis=1)]\n",
    "df['anger']=scores[:,0]\n",
    "df['joy']=scores[:,1]\n",
    "df['optimism']=scores[:,2]\n",
    "df['sadness']=scores[:,3]\n",
    "\n",
    "\n",
    "# for printing\n",
    "for t in range(3):\n",
    "    print(f'text: {df.text.iloc[t]}')\n",
    "    for i in range(scores[0].shape[0]):\n",
    "        l = labels[ranking[t][i]]\n",
    "        s = scores[t][ranking[t][i]]\n",
    "        print(f\"{i+1}) {l} {np.round(float(s), 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sponsor me @Crocs @Nike https://t.co/SQpbd2x2MC'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('tweets.csv',sep=';',index=False, quotechar='\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8d0e0863b4dbc785a2b1bd227b176323538a9cabeeaebcabf04a1216aba68dd7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
